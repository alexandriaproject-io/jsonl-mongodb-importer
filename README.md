# jsonl-mongodb-importer

Imports jsonl datasets to mongodb with remapping

Example

```bash
$: python3 import_togethercomupter_dataset.py [dataset_id] [dataset_file_path]
# $: python3 import_togethercomupter_dataset.py 123123123 ./files/together-dataset.jsonl
```

Each dataset is different so this project will contain a collection of scripts each for its own type of dataset. \
The datasets will be coverted to a strict data structure as follows: \
`NOTE: nothing is final and the structure may change as we encounter weireder datasets in the future`

### Dataset Source Card

```json
{
  "_id": "Mongo ObjectId",
  "datasetName": "String - Name of the dataset",
  "datasetUrl": "String - Dataset URL for reference",
  "datasetType": "String - Training data type: One of['llm-chat','llm-instruct','llm-completion', 'llm-analyze']",
  "datasetTargetLength": "String - Target context size: One of ['4k','8k','16k','32k','64k','128k']",
  "datasetOrigin": "String - Data origin: One of ['human','generated','mixed']"
}
```

**Dataset Type**

- llm-chat - Contains chat conversations between user and assistant
- llm-instruct - Contains user instruction and assistant response (can be multiple)
- llm-completion - Standard complete text type dataset
- llm-analyze - Dataset contains training data to allow the LLM to analyze the data and output a response ( book summary
  for example )

**Dataset Target Length**

- The length is context approximation to avoid large context datasets to get mixed in with smaller ones

**Origin types:**

- human - The dataset is fully created by humans
- generated - The dataset is fully generated by AI
- mixed - Some of the data is human made and some is AI

NOTE: datacard should be created manually in the DB ( the ID is required for the data items remapping )

### Dataset original jsonl row

Some datasets may require original reference, especially the multi message ones we will be splitting to their own lines

```json
{
  "_id": "Mongo ObjectId",
  "datasetId": "Mongo ObjectId@index - Id of the dataset card",
  "rawJsonString": "String - the original line of the jsonl",
  "rawJsonHash": "String@index@unique - SHA256 of the original line to avoid duplicates"
}
```

### Dataset data row structure

We want the same format for all of the dataset items regardless of their original structure to make selecting or working
with them easier

```json
{
  "_id": "Mongo ObjectId",
  "datasetId": "Mongo ObjectId@index - Id of the dataset card",
  "rawJsonId": "Mongo ObjectId@index@null - Id of the original data ( can be null )",
  "systemMessage": "String@empty - System message for the LLM",
  "contextMessages": [
    "Context message for training row",
    "Another context message for training row",
    ...
  ],
  "state": "String@default('new') - Data adjustments/completion may require special states",
  "seriesPosition": "Number - in multiple chat messages what is the positions",
  "messages": [
    {
      "provider": "String - One of ['user','assistant','memory','segment']",
      "message": "String"
    },
    {
      "provider": "String - One of ['user','assistant','memory','segment']",
      "message": "String"
    },
    ...
  ]
}
```

**Provider types**

- user - Message was created by user
- assistant - Message was created by the LLM or assistant
- memory - Message was taken from a vector DB
- segment - The default option for texts or analyze type llm training sets

# Dataset enhancement jobs

Example

```bash
$: python3 job-creator [dataset_id] [job-type] [target] using [source] [source] [source] with [tool]
# $: python3 job-creator 12312312 generate system using messages context with gpt-4
```

## Dataset data row job / extention / modification

```json
{
  "_id": "Mongo ObjectId",
  "datasetId": "Mongo ObjectId@index - Id of the dataset card",
  "jobHash": "String@index@unique - sha256 of job row, type, target and source to avoid duplicates",
  "datasetRowId": "Mongo ObjectId - id of target dataset row",
  "type": "String - What to do: One of ['generate', 'expand', 'rephrase']",
  "target": "String - What to create/edit: One of ['messages', 'system','context']",
  "sources": "Array of String['messages', 'system','context']",
  "status": "String@default('new') - Job status: One of ['new','queued','error','completed']",
  "tool": "String - What tool to use: One of ['default','gpt-3.5', 'gpt-4', 'gpt-4-1106-preview','llama-7b', 'llama-7b-chat','llama-13b', 'llama-13b-chat']",
  "completedDate": "Date@not-set - when set TTL of 7 days for auto deletion ( if not set should not be removed )"
}
```

**Job types**

- generate - Generate a new text for the target, if target is not empty the job will be marked as done
- expand - Generate additional text or details for the target and replace the target with the new text
- rephrase - Change the target wording ( useful to avoid over-fitting a model)

`NOTE: There will be more specific jobs added over time`

## Virtual environment activation

windows:

```
.\env\Scripts\activate
```

Linux

```
source env/bin/activate

```